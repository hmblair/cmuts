#!/usr/bin/env python3

import h5py
import dask.array as da
import dask
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
import os
import argparse
import numpy as np

VERSION = '1.0.0'
NAME = 'cmuts-normalize'
PBAR_WIDTH = 10

REACTIVITY_DS = 'reactivity'
READS_DS = 'reads'
ERROR_DS = "error"
SNR_DS = "snr"
NORM_DS = "norm"
ROI_DS = "ROI"
SEQUENCE_DS = 'sequence'

NORM_CUTOFF = 50
NORM_PERCENTILE = 90


def _all_unique(x: list):
    return len(set(x)) == len(x)


parser = argparse.ArgumentParser()
parser.add_argument(
    'file',
    help='The HDF5 file containing the input data.',
)
parser.add_argument(
    '--mod-ds',
    help='The names of the datasets containing the mutation counts of the modified sequences.',
    nargs="+",
)
parser.add_argument(
    '--nomod-ds',
    help='The names of the datasets containing the mutation counts of the non-modified sequences.',
    nargs="+",
)
parser.add_argument(
    '--out-groups',
    help='The groups in the output file to place the reads and reactivities.',
    nargs="+",
)
parser.add_argument(
    '-o', '--out',
    help='The output HDF5 file to write to.',
    default='reactivity.h5'
)
parser.add_argument(
    '--overwrite',
    help='Overwrite the existing HDF5 file.',
    action='store_true',
)
parser.add_argument(
    '-t', '--threads',
    help='The number of threads or processes to use.',
    type=int,
    default=1,
)
parser.add_argument(
    '--clip-reactivity',
    help='Clip the reactivity values to the range [0,1].',
    action='store_true',
)
parser.add_argument(
    '--5p-primer-length',
    help='The length of the 5\' primer, which will be zeroed out.',
    type=int,
    default=26,
)
parser.add_argument(
    '--3p-primer-length',
    help='The length of the 3\' primer, which will be zeroed out.',
    type=int,
    default=20,
)


def _title(name: str, version: str) -> str:
    return f"""        {name} version {version}
      ───────────────────────────────────"""


def _remove_if_exists(
    path: str,
    overwrite: bool = False
) -> None:
    if os.path.exists(path):
        if overwrite:
            os.remove(path)
        else:
            raise FileExistsError(
                f"The file \"{path}\" already exists. Please set the"
                " --overwrite flag to overwrite the existing file."
            )


def _throw_if_bad_names(
    mod_ds: list[str],
    nomod_ds: list[str | None],
    out_groups: list[str],
) -> None:
    if len(mod_ds) != len(out_groups):
        raise ValueError(
            "There must be an equal number of input datasets "
            "and output groups."
        )

    if len(mod_ds) != len(nomod_ds):
        raise ValueError(
            "There must be an equal number of mod and "
            "nomod datasets if the latter is specified."
        )

    if not _all_unique(out_groups):
        raise ValueError("Each output group must be distinct.")


def _reactivity(modifications: da.Array, coverage: da.Array) -> da.Array:

    return da.divide(
        modifications,
        coverage,
        where=(coverage > 0),
        out=da.zeros_like(modifications),
    )


def _error(reactivity: da.Array, coverage: da.Array) -> da.Array:

    return da.divide(
        da.sqrt(reactivity * (1 - reactivity)),
        da.sqrt(coverage),
        where=(coverage > 0),
        out=da.ones_like(reactivity),
    )


def _snr(reactivity: da.Array, error: da.Array) -> da.Array:

    return da.divide(
        reactivity,
        error,
        where=(error > 0),
        out=da.zeros_like(reactivity),
    ).mean(-1)


def _process(file: h5py.File, group: str) -> tuple[da.Array, ...]:

    # Lazily load the HDF5 files into dask arrays

    arr = da.from_array(file[group], chunks='auto')

    # Compute the coverage as the sum of all mutated, non-mutated, and
    # deleted bases

    coverage = da.sum(arr[..., :-1], axis=(2, 3))

    # Compute the trace to get the non-modified rates only

    trace = da.trace(arr, axis1=2, axis2=3)

    # Compute the insertions

    ins = da.sum(arr[..., -1], axis=-1)

    # Subtract the trace to get the pure modifications

    modifications = coverage + ins - trace

    # Divide by the coverage to get the un-normalised reactivity

    reactivity = _reactivity(modifications, coverage)

    # SEM of a Bernoulli random variable

    err = _error(reactivity, coverage)

    # Blank out the ends of the sequence

    reactivity[:, :BLANK_OUT5] = 0
    reactivity[:, BLANK_OUT3:] = 0
    err[:, :BLANK_OUT5] = 0
    err[:, BLANK_OUT3:] = 0

    # Get the read counts

    reads = da.max(coverage, axis=(-1,))

    return reactivity, reads, err


def normalise(
    reactivity: da.Array,
    reads: da.Array,
) -> da.Array:

    # Get the reactivity of all high-read sequences

    high_reactivity = reactivity[reads > NORM_CUTOFF]

    # Find the 90th percentile of these reactivity values

    return da.percentile(
        high_reactivity[:, UNBLANKED].flatten(),
        NORM_PERCENTILE,
    )


def _get_sequences(file: h5py.File) -> da.Array | None:
    # Get the tokenized sequences if they exist
    if SEQUENCE_DS in file:
        return da.from_array(file[SEQUENCE_DS], chunks='auto')

    return None


if __name__ == '__main__':

    args = parser.parse_args()
    print(_title(NAME, VERSION))

    _remove_if_exists(args.out, args.overwrite)

    if args.nomod_ds is None:
        args.nomod_ds = len(args.mod_ds) * [None]

    _throw_if_bad_names(args.mod_ds, args.nomod_ds, args.out_groups)

    pbar = ProgressBar(width=PBAR_WIDTH)
    pbar.register()

    with h5py.File(args.file, 'r') as f:

        names = zip(
            args.mod_ds,
            args.nomod_ds,
            args.out_groups
        )
        for mod_ds, nomod_ds, out_group in names:

            BLANK_OUT5 = getattr(args, "5p_primer_length")
            BLANK_OUT3 = f[mod_ds].shape[1] - getattr(args, "3p_primer_length")
            UNBLANKED = slice(BLANK_OUT5, BLANK_OUT3)

            mod_reactivity, reads, err = _process(f, mod_ds)

            if nomod_ds is not None:
                nomod_reactivity, nomod_reads, no_mod_err = _process(f, nomod_ds)
                reads += nomod_reads
                err = da.sqrt(err ** 2 + no_mod_err ** 2)

            # We need the reads computed in advance in order to
            # find all sequences with more than NORM_CUTOFF reads

            _reads = reads.compute()

            # Normalise the reactivity and its error
            # SNR is unchanged

            norm = normalise(mod_reactivity, _reads)
            err /= norm

            # Background subtract

            if nomod_ds is not None:
                reactivity = (mod_reactivity - nomod_reactivity) / norm
            else:
                reactivity = mod_reactivity / norm

            snr = _snr(reactivity, err)

            # Clip the reactivity if requested

            if args.clip_reactivity:
                reactivity = da.clip(reactivity, 0, 1)

            # Find the region of interest

            roi = da.array([BLANK_OUT5, reactivity.shape[1] + BLANK_OUT3])

            # Save to file

            data = {
                out_group + '/' + REACTIVITY_DS: reactivity,
                out_group + '/' + READS_DS: reads,
                out_group + '/' + NORM_DS: norm,
                out_group + '/' + ROI_DS: roi,
                out_group + '/' + ERROR_DS: err,
                out_group + '/' + SNR_DS: snr,
            }

            da.to_hdf5(args.out, data)

        # Get the embedded sequences

        sequences = _get_sequences(f)
        if sequences is not None:

            # Save to file

            aux_data = {
                SEQUENCE_DS: sequences,
            }
            da.to_hdf5(args.out, aux_data)
