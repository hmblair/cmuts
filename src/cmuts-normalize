#!/usr/bin/env python3

import h5py
import dask.array as da
import dask
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
import os
import argparse
import numpy as np

VERSION = '1.0.0'
NAME = 'normalize'
PBAR_WIDTH = 10

REACTIVITY_DS = 'reactivity'
READS_DS = 'reads'
SEQUENCE_DS = 'sequence'
ROI_DS = "ROI"

NORM_CUTOFF = 500
NORM_PERCENTILE = 90


def _all_unique(x: list):
    return len(set(x)) == len(x)


parser = argparse.ArgumentParser()
parser.add_argument(
    'file',
    help='The HDF5 file containing the input data.',
)
parser.add_argument(
    '--mod-ds',
    help='The names of the datasets containing the mutation counts of the modified sequences.',
    nargs="+",
)
parser.add_argument(
    '--nomod-ds',
    help='The names of the datasets containing the mutation counts of the non-modified sequences.',
    nargs="+",
)
parser.add_argument(
    '--out-groups',
    help='The groups in the output file to place the reads and reactivities.',
    nargs="+",
)
parser.add_argument(
    '-o', '--out',
    help='The output HDF5 file to write to.',
    default='reactivity.h5'
)
parser.add_argument(
    '--overwrite',
    help='Overwrite the existing HDF5 file.',
    action='store_true',
)
parser.add_argument(
    '-t', '--threads',
    help='The number of threads or processes to use.',
    type=int,
    default=1,
)
parser.add_argument(
    '--clip-reactivity',
    help='Clip the reactivity values to the range [0,1].',
    action='store_true',
)
parser.add_argument(
    '--5p-primer-length',
    help='The length of the 5\' primer, which will be zeroed out.',
    type=int,
    default=26,
)
parser.add_argument(
    '--3p-primer-length',
    help='The length of the 3\' primer, which will be zeroed out.',
    type=int,
    default=20,
)


def get_reads_and_reactivity(arr: da.Array) -> tuple[da.Array, da.Array]:
    # Compute the coverage as the sum of all mutated, non-mutated, and
    # deleted bases
    coverage = da.sum(arr[..., :-1], axis=(-2, -1))
    # Compute the trace to get the non-mutated bases only
    trace = da.trace(arr, axis1=2, axis2=3)
    # Compute the insertions
    ins = da.sum(arr[..., -1], axis=-1)
    # Subtract the trace to get the mutation and deletion rates
    mut_rates = coverage + ins - trace
    # Divide by the coverage to get the un-normalised reactivity
    reactivity = da.divide(
        mut_rates,
        coverage,
        where=(coverage > 0),
        out=np.zeros_like(mut_rates),
    )
    # Blank out the ends of the sequence
    reactivity[:, :BLANK_OUT5] = 0
    reactivity[:, BLANK_OUT3:] = 0
    # Get the read counts
    reads = da.max(coverage, axis=(-1,))

    return reactivity, reads


def normalise(
    reactivity: da.Array,
    reads: da.Array,
) -> da.Array:
    high_reactivity = reactivity[reads > NORM_CUTOFF]
    # Find the 90th percentile of the reactivity values among the high-read sequences
    norm = da.percentile(
        high_reactivity[:, UNBLANKED].flatten(),
        NORM_PERCENTILE,
    )
    # Normalise the reactivity by this value
    return reactivity / norm


def _process(file: h5py.File, group: str) -> tuple[da.Array, da.Array]:
    # Lazily load the HDF5 files into dask arrays
    arr = da.from_array(file[group], chunks='auto')
    # Compute the un-normalised reactivity and the read counts
    return get_reads_and_reactivity(arr)


def _get_sequences(file: h5py.File):
    return da.from_array(file[SEQUENCE_DS], chunks='auto')


def _remove_if_exists(
    path: str,
    overwrite: bool = False
):
    if os.path.exists(path):
        if overwrite:
            os.remove(path)
        else:
            raise FileExistsError(
                f"The file \"{path}\" already exists. Please set the"
                " --overwrite flag to overwrite the existing file."
            )


def _throw_if_bad_names(
    mod_ds: list[str],
    nomod_ds: list[str] | None,
    out_groups: list[str],
):
    if len(mod_ds) != len(out_groups):
        raise ValueError(
            "There must be an equal number of input datasets "
            "and output groups."
        )

    if nomod_ds is not None and len(mod_ds) != len(nomod_ds):
        raise ValueError(
            "There must be an equal number of mod and "
            "nomod datasets if the latter is specified."
        )

    if not _all_unique(out_groups):
        raise ValueError("Each output group must be distinct.")


if __name__ == '__main__':

    args = parser.parse_args()

    _remove_if_exists(args.out, args.overwrite)
    _throw_if_bad_names(args.mod_ds, args.nomod_ds, args.out_groups)

    if args.nomod_ds is None:
        args.nomod_ds = len(args.mod_ds) * [None]

    BLANK_OUT5 = getattr(args, "5p_primer_length")
    BLANK_OUT3 = -getattr(args, "3p_primer_length")
    UNBLANKED = slice(BLANK_OUT5, BLANK_OUT3)

    with h5py.File(args.file, 'r') as f:

        _names = zip(
            args.mod_ds,
            args.nomod_ds,
            args.out_groups
        )
        for mod_ds, nomod_ds, out_group in _names:

            mod_reactivity, mod_reads = _process(f, mod_ds)
            reads = mod_reads
            if nomod_ds:
                nomod_reactivity, nomod_reads = _process(f, nomod_ds)
                reads += nomod_reads

            with ProgressBar(width=PBAR_WIDTH):
                # We need the reads computed in advance in order to
                # find all sequences with more than NORM_CUTOFF reads
                _reads = reads.compute()
                # Normalise the reactivity
                norm_mod_reactivity = normalise(mod_reactivity, _reads)
                if nomod_ds:
                    norm_nomod_reactivity = normalise(nomod_reactivity, _reads)
                    reactivity = norm_mod_reactivity - norm_nomod_reactivity
                else:
                    reactivity = norm_mod_reactivity
                # Clip the reactivity if requested
                if args.clip_reactivity:
                    reactivity = da.clip(reactivity, 0, 1)

                # Get the embedded sequences
                sequences = _get_sequences(f)
                # Track the region of interest
                roi = da.array([BLANK_OUT5, sequences.shape[1] + BLANK_OUT3])
                # Save to file
                data = {
                    out_group + '/' + REACTIVITY_DS: reactivity,
                    out_group + '/' + READS_DS: reads,
                }
                da.to_hdf5(args.out, data)

        # Get the embedded sequences
        sequences = _get_sequences(f)
        # Track the region of interest
        roi = da.array([BLANK_OUT5, sequences.shape[1] + BLANK_OUT3])
        # Save to file
        aux_data = {
            SEQUENCE_DS: sequences,
            ROI_DS: roi
        }
        da.to_hdf5(args.out, aux_data)
