#!/usr/bin/env python3

import h5py
import dask.array as da
import dask
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
import os
import argparse
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm

VERSION = '1.0.0'
NAME = 'cmuts-normalize'
PBAR_WIDTH = 10

REACTIVITY_DS = 'reactivity'
READS_DS = 'reads'
ERROR_DS = "error"
SNR_DS = "snr"
NORM_DS = "norm"
ROI_DS = "ROI"
SEQUENCE_DS = 'sequence'


def _all_unique(x: list):
    return len(set(x)) == len(x)


parser = argparse.ArgumentParser()
parser.add_argument(
    'file',
    help='The HDF5 file containing the input data.',
)
parser.add_argument(
    '--mod',
    help='The name of the dataset containing the mutation counts of the modified sequences.',
    required=True,
)
parser.add_argument(
    '--nomod',
    help='The name of the dataset containing the mutation counts of the non-modified sequences.',
)
parser.add_argument(
    '--group',
    help='The group in the output file to place the reads and reactivities.',
    required=True,
)
parser.add_argument(
    '-o', '--out',
    help='The output HDF5 file to write to.',
    default='reactivity.h5'
)
parser.add_argument(
    '--overwrite',
    help='Overwrite an existing HDF5 file (the whole file, not just the group).',
    action='store_true',
)
parser.add_argument(
    '-t', '--threads',
    help='The number of threads or processes to use.',
    type=int,
    default=1,
)
parser.add_argument(
    '--clip-reactivity',
    help='Clip the reactivity values to the range [0,1].',
    action='store_true',
)
parser.add_argument(
    '--5p-primer-length',
    help='The length of the 5\' primer, which will be zeroed out.',
    type=int,
    default=0,
)
parser.add_argument(
    '--3p-primer-length',
    help='The length of the 3\' primer, which will be zeroed out.',
    type=int,
    default=0,
)
parser.add_argument(
    '--raw',
    help='Do not normalize the reactivity values.',
    action='store_true',
)
parser.add_argument(
    '--norm-cutoff',
    help='References with at least this many reads are used for normalization.',
    type=int,
    default=500,
)
parser.add_argument(
    '--norm-percentile',
    help='The reactivity percentile to use for normalization.',
    type=int,
    default=90,
)


def _title(name: str, version: str) -> str:
    return f"""        {name} version {version}
      ───────────────────────────────────"""


def _remove_if_exists(
    path: str,
    overwrite: bool = False
) -> None:
    if os.path.exists(path):
        if overwrite:
            os.remove(path)


def _reactivity(modifications: da.Array, coverage: da.Array) -> da.Array:

    return da.divide(
        modifications,
        coverage,
        where=(coverage > 0),
        out=da.zeros_like(modifications),
    )


def _error(reactivity: da.Array, coverage: da.Array) -> da.Array:

    return da.divide(
        da.sqrt(reactivity * (1 - reactivity)),
        da.sqrt(coverage),
        where=(coverage > 0),
        out=da.ones_like(reactivity),
    )


def _snr(reactivity: da.Array, error: da.Array) -> da.Array:

    return da.divide(
        reactivity,
        error,
        where=(error > 0),
        out=da.zeros_like(reactivity),
    ).mean(-1)


def _heatmap(file: h5py.File, group: str) -> tuple[da.Array, ...]:

    # Lazily load the HDF5 files into dask arrays

    try:
        arr = da.from_array(file[group], chunks='auto')
    except Exception as e:
        print(f"Error loading the dataset {group}:")
        raise e

    return 


def _process(file: h5py.File, group: str) -> tuple[da.Array, ...]:

    # Lazily load the HDF5 files into dask arrays

    try:
        arr = da.from_array(file[group], chunks='auto')
    except Exception as e:
        print(f"Error loading the dataset {group}:")
        raise e

    # Compute the heatmap

    heatmap = arr.mean((0, 1))
    heatmap = heatmap / heatmap.sum()

    # Compute the coverage as the sum of all mutated, non-mutated, and
    # deleted bases

    coverage = da.sum(arr[..., :-1], axis=(2, 3))

    # Compute the trace to get the non-modified rates only

    trace = da.trace(arr, axis1=2, axis2=3)

    # Compute the insertions

    ins = da.sum(arr[..., -1], axis=-1)

    # Subtract the trace to get the pure modifications

    modifications = coverage + ins - trace

    # Divide by the coverage to get the un-normalised reactivity

    reactivity = _reactivity(modifications, coverage)

    # SEM of a Bernoulli random variable

    err = _error(reactivity, coverage)

    # Blank out the ends of the sequence

    reactivity[:, :BLANK_OUT5] = 0
    reactivity[:, BLANK_OUT3:] = 0
    err[:, :BLANK_OUT5] = 0
    err[:, BLANK_OUT3:] = 0

    # Get the read counts

    reads = da.max(coverage, axis=(-1,))

    return reactivity, reads, err, heatmap


def normalise(
    reactivity: da.Array,
    reads: da.Array,
    cutoff: int,
    percentile: int
) -> da.Array:

    # Get the reactivity of all high-read sequences

    high_reactivity = reactivity[reads > cutoff]

    # Find the 90th percentile of these reactivity values

    return da.percentile(high_reactivity[:, UNBLANKED].flatten(), percentile)


def _get_sequences(file: h5py.File) -> da.Array:
    # Get the tokenized sequences if they exist
    if SEQUENCE_DS in file:
        return da.from_array(file[SEQUENCE_DS], chunks='auto')

    return None


def _plot_heatmap(heatmap: da.Array, name: str) -> None:

    im = plt.imshow(heatmap, cmap="RdPu", norm=LogNorm(vmin=1E-4, vmax=1E0))
    plt.colorbar(im)

    plt.title("Modification Heatmap", fontsize=15)
    plt.xticks(range(6), ["A", "C", "G", "U", "del", "ins"], fontsize=12)
    plt.yticks(range(4), ["A", "C", "G", "U"], fontsize=12)

    plt.savefig(f"figures/{name}-heatmap.png", dpi=300, bbox_inches="tight")
    plt.close()


if __name__ == '__main__':

    args = parser.parse_args()
    print(_title(NAME, VERSION))

    _remove_if_exists(args.out, args.overwrite)

    pbar = ProgressBar(width=PBAR_WIDTH)
    pbar.register()

    with h5py.File(args.file, 'r') as f:

        BLANK_OUT5 = getattr(args, "5p_primer_length")
        BLANK_OUT3 = f[args.mod].shape[1] - getattr(args, "3p_primer_length")
        UNBLANKED = slice(BLANK_OUT5, BLANK_OUT3)

        mod_reactivity, reads, err, heatmap = _process(f, args.mod)

        if args.nomod is not None:
            nomod_reactivity, nomod_reads, no_mod_err, no_mod_heatmap = _process(f, args.nomod)
            reads += nomod_reads
            err = da.sqrt(err ** 2 + no_mod_err ** 2)

        # We need the reads computed in advance in order to
        # find all sequences with more than NORM_CUTOFF reads

        _reads = reads.compute()

        # Normalise the reactivity and its error
        # SNR is unchanged

        if args.raw:
            norm = da.ones(1, dtype=reads.dtype)
        else:
            norm = normalise(
                mod_reactivity,
                _reads,
                args.norm_cutoff,
                args.norm_percentile,
            )

        err /= norm

        # Background subtract

        if args.nomod is not None:
            reactivity = (mod_reactivity - nomod_reactivity) / norm
        else:
            reactivity = mod_reactivity / norm

        snr = _snr(reactivity, err)

        # Clip the reactivity if requested

        if args.clip_reactivity:
            reactivity = da.clip(reactivity, 0, 1)

        # Find the region of interest

        roi = da.array([BLANK_OUT5, reactivity.shape[1] + BLANK_OUT3])

        # Save to file

        data = {
            args.group + '/' + REACTIVITY_DS: reactivity,
            args.group + '/' + READS_DS: reads,
            args.group + '/' + NORM_DS: norm,
            args.group + '/' + ROI_DS: roi,
            args.group + '/' + ERROR_DS: err,
            args.group + '/' + SNR_DS: snr,
        }

        da.to_hdf5(args.out, data)

        # Plot heatmap

        os.makedirs("figures")
        _plot_heatmap(heatmap, args.group)

        heatmap[0, 0] = heatmap[1, 1] = heatmap[2, 2] = heatmap[3, 3] = 0
        mods = heatmap.sum(0).compute()
        print(mods)

        # Get the embedded sequences

        sequences = _get_sequences(f)

        if sequences is not None:

            aux_data = {
                SEQUENCE_DS: sequences,
            }
            da.to_hdf5(args.out, aux_data)
