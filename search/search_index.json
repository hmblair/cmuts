{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#cmuts","title":"cmuts","text":"<p>Fast mutation counting and reactivity profiling for MaP-seq experiments.</p> <p></p>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install all requirements</li> <li>Build and install cmuts</li> <li>Run the example under <code>./examples/map-seq</code></li> <li>Copy the <code>./COMMANDS</code> file and modify as necessary for your inputs</li> </ol>"},{"location":"architecture/","title":"Architecture Overview","text":""},{"location":"architecture/#architecture-overview","title":"Architecture Overview","text":"<p>This document describes the high-level architecture of cmuts for developers and maintainers.</p>"},{"location":"architecture/#data-flow-pipeline","title":"Data Flow Pipeline","text":"<pre><code>FASTQ \u2192 Alignment \u2192 BAM/CRAM \u2192 cmuts-core \u2192 HDF5 \u2192 cmuts-normalize \u2192 Reactivity\n</code></pre>"},{"location":"architecture/#stage-1-mutation-counting-c","title":"Stage 1: Mutation Counting (C++)","text":"<p>The C++ core (<code>cmuts-core</code>) processes aligned reads to count mutations:</p> <ol> <li>Input Parsing: Reads BAM/CRAM files using htslib with streaming I/O</li> <li>Reference Loading: Loads FASTA reference sequences into memory</li> <li>Mutation Detection: For each aligned read:</li> <li>Parses CIGAR string to identify mismatches, insertions, deletions</li> <li>Records mutation type and position in count matrices</li> <li>Handles ambiguous deletions using mutation-informed heuristics</li> <li>Output: Writes HDF5 files containing count tensors</li> </ol>"},{"location":"architecture/#stage-2-normalization-python","title":"Stage 2: Normalization (Python)","text":"<p>The Python package (<code>cmuts-normalize</code>) transforms counts to reactivity:</p> <ol> <li>Count Aggregation: Combines counts across replicates/conditions</li> <li>Coverage Filtering: Masks positions below coverage threshold</li> <li>Reactivity Calculation: <code>reactivity = modifications / coverage</code></li> <li>Normalization: Applies scheme (raw, percentile, or outlier-based)</li> <li>Pairwise Analysis: Computes correlation matrices and mutual information</li> </ol>"},{"location":"architecture/#module-responsibilities","title":"Module Responsibilities","text":""},{"location":"architecture/#c-core-srccpp","title":"C++ Core (<code>src/cpp/</code>)","text":"File Responsibility <code>main.cpp</code> Entry point, argument parsing <code>bam.cpp</code> BAM file reading and iteration <code>cram.cpp</code> CRAM file reading with reference handling <code>cmuts.cpp</code> Core mutation counting logic <code>hdf5.cpp</code> HDF5 output with chunking and compression <code>mpi.cpp</code> MPI coordination for distributed processing <code>fasta.cpp</code> FASTA reference sequence parsing <code>utils.cpp</code> Utility functions"},{"location":"architecture/#python-package-srcpythoncmuts","title":"Python Package (<code>src/python/cmuts/</code>)","text":"Module Responsibility <code>internal.py</code> Core data structures (<code>ProbingData</code>, <code>Opts</code>) and normalization <code>normalize/</code> CLI for reactivity normalization <code>visualize/</code> Plotting and visualization tools"},{"location":"architecture/#data-structures","title":"Data Structures","text":""},{"location":"architecture/#hdf5-count-file-structure","title":"HDF5 Count File Structure","text":"<pre><code>/\n\u251c\u2500\u2500 sequences          # Tokenized reference sequences (optional)\n\u251c\u2500\u2500 mod/               # Modified condition\n\u2502   \u251c\u2500\u2500 counts-1d      # Shape: (refs, seqlen, 4, 7) - 1D mutation counts\n\u2502   \u2514\u2500\u2500 counts-2d      # Shape: (refs, seqlen, seqlen, 2, 2) - pairwise counts\n\u2514\u2500\u2500 nomod/             # Unmodified control (optional)\n    \u251c\u2500\u2500 counts-1d\n    \u2514\u2500\u2500 counts-2d\n</code></pre>"},{"location":"architecture/#1d-count-tensor-axes","title":"1D Count Tensor Axes","text":"<ul> <li>Axis 0: Reference sequences</li> <li>Axis 1: Sequence positions</li> <li>Axis 2: Reference nucleotide (A, C, G, T)</li> <li>Axis 3: Observation type (A, C, G, T, del, ins, term)</li> </ul>"},{"location":"architecture/#2d-count-tensor-axes","title":"2D Count Tensor Axes","text":"<ul> <li>Axes 0-2: Same as 1D (ref, pos_i, pos_j)</li> <li>Axes 3-4: Binary mutation state at each position (0=match, 1=mutation)</li> </ul>"},{"location":"architecture/#mpi-coordination-model","title":"MPI Coordination Model","text":"<p>When built with MPI support, cmuts distributes work across ranks:</p> <ol> <li>Rank 0 (Coordinator):</li> <li>Opens input BAM/CRAM file</li> <li>Distributes reference sequence assignments to workers</li> <li> <p>Aggregates final results and writes output</p> </li> <li> <p>Worker Ranks:</p> </li> <li>Receive assigned reference sequences</li> <li>Process reads mapping to assigned references</li> <li> <p>Send local counts back to coordinator</p> </li> <li> <p>Load Balancing:</p> </li> <li>References assigned by estimated read count</li> <li>Larger references may be split across multiple ranks</li> </ol>"},{"location":"architecture/#extension-points","title":"Extension Points","text":""},{"location":"architecture/#adding-new-mutation-types","title":"Adding New Mutation Types","text":"<ol> <li>Extend the observation type enum in <code>src/cpp/cmuts.hpp</code></li> <li>Update CIGAR parsing in <code>cmuts.cpp</code> to detect new type</li> <li>Modify HDF5 schema and Python loader</li> </ol>"},{"location":"architecture/#adding-new-normalization-schemes","title":"Adding New Normalization Schemes","text":"<ol> <li>Add enum value to <code>NormScheme</code> in <code>internal.py</code></li> <li>Implement <code>_get_norm_&lt;scheme&gt;()</code> function</li> <li>Update <code>_get_norm()</code> dispatch logic</li> </ol>"},{"location":"architecture/#adding-new-output-formats","title":"Adding New Output Formats","text":"<ol> <li>Create new writer class in <code>src/cpp/</code></li> <li>Add CLI flag in argument parser</li> <li>Wire up in <code>main.cpp</code></li> </ol>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Memory: Count tensors can be large; use chunked HDF5 I/O</li> <li>I/O Bound: BAM parsing is typically the bottleneck</li> <li>Thread Safety: OpenMP parallelization requires thread-local accumulators</li> <li>MPI Overhead: Small references may not benefit from distribution</li> </ul>"},{"location":"architecture/#dependencies","title":"Dependencies","text":""},{"location":"architecture/#c-system","title":"C++ (System)","text":"<ul> <li>htslib: BAM/CRAM parsing</li> <li>HDF5: Output format</li> <li>zlib: Compression</li> <li>OpenMP: Threading</li> <li>MPI (optional): Distributed computing</li> </ul>"},{"location":"architecture/#c-fetched","title":"C++ (Fetched)","text":"<ul> <li>xtensor: N-dimensional arrays</li> <li>xtl: Template utilities</li> <li>argparse: CLI parsing</li> </ul>"},{"location":"architecture/#python","title":"Python","text":"<ul> <li>numpy, scipy: Numerical computing</li> <li>dask: Lazy array operations</li> <li>h5py: HDF5 file access</li> <li>matplotlib: Visualization</li> <li>biopython: Sequence handling</li> </ul>"},{"location":"installation/installation/","title":"Installation","text":""},{"location":"installation/installation/#installation","title":"Installation","text":""},{"location":"installation/installation/#1-obtain-cmuts","title":"1. Obtain <code>cmuts</code>","text":"<p>Clone the <code>cmuts</code> repository with all submodules:</p> <pre><code>git clone --recurse-submodules https://github.com/hmblair/cmuts\ncd cmuts\n</code></pre>"},{"location":"installation/installation/#2-build-and-install-cmuts","title":"2. Build and Install <code>cmuts</code>","text":"<p>The configuration script contains all required setup and build steps.</p> <p>Warning</p> <p>This will also install the <code>cmuts</code> wheel via <code>pip</code>, so if needed enter your environment of choice first.</p> <pre><code>./configure\n</code></pre> <p>If building the multithreaded version, pass the <code>--mpi</code> flag to the script.</p> <pre><code>./configure --mpi\n</code></pre> <p>A debug build, which reduces optimization and enables various sanitizers, is also possible.</p> <pre><code>./configure --debug\n</code></pre> <p>Warning</p> <p>Building with <code>--mpi</code> or <code>--debug</code> after an existing build has already been performed will require deleting the <code>build</code> directory first. This can equivalently be done by passing the <code>--clean</code> flag to the script.</p>"},{"location":"installation/installation/#3-modify-path","title":"3. Modify PATH","text":"<p>Afterwards, add the <code>cmuts</code> binary directory to your PATH:</p> Temporary (current session)Permanent (bash)Permanent (zsh) <pre><code>export PATH=\"$(pwd)/bin:$PATH\"\n</code></pre> <pre><code>echo 'export PATH=\"'$(pwd)'/bin:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <pre><code>echo 'export PATH=\"'$(pwd)'/bin:$PATH\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n</code></pre>"},{"location":"installation/installation/#updating","title":"Updating","text":"<p>To update to a newer version, from within the <code>cmuts</code> directory, run</p> <pre><code>git pull origin master\ngit submodule update --recursive\n./configure --clean # OR ./configure --clean --mpi\n</code></pre>"},{"location":"installation/org-notes/","title":"Organization-Specific Notes","text":"<p>The following is an incomplete list of various locations where <code>cmuts</code> has been run, and the specific requirements for each one.</p>"},{"location":"installation/org-notes/#stanford-sherlock-cluster","title":"Stanford Sherlock Cluster","text":"<p>Both building and running will require the following, which covers all dependencies.</p> <pre><code>ml load hdf5/1.14.4\nml load biology samtools/1.16.1\nml load cmake/3.31.4\n</code></pre>"},{"location":"installation/org-notes/#hhmi-janelia-cluster","title":"HHMI Janelia Cluster","text":"<p>Running will require the following two commands.</p> <pre><code>ml load samtools\nLD_LIBRARY_PATH=/misc/local/samtools-1.22.1/lib:$LD_LIBRARY_PATH\n</code></pre> <p>In addition, building <code>cmuts</code> will require</p> <pre><code>ml load cmake/4.0.2\nexport PKG_CONFIG_PATH=/misc/local/samtools-1.22.1/lib/pkgconfig:$PKG_CONFIG_PATH\n</code></pre>"},{"location":"installation/requirements/","title":"Requirements","text":""},{"location":"installation/requirements/#dependencies","title":"Dependencies","text":"<p>Building and running the <code>cmuts</code> pipeline requires the following packages:</p> <ul> <li><code>python&gt;=3.9</code> with all packages in <code>requirements.txt</code></li> <li><code>cmake&gt;=3.29</code> with <code>pkg-config</code></li> <li><code>autoconf</code></li> <li><code>samtools</code> and <code>htslib</code></li> <li><code>hdf5</code></li> <li><code>omp</code></li> <li><code>fastp</code></li> </ul> <p>The MPI build also requires</p> <ul> <li><code>openmpi</code></li> <li><code>hdf5-mpi</code></li> </ul> <p>In addition, demultiplexing with <code>cmuts align</code> requires</p> <ul> <li><code>ultraplex</code></li> </ul> <p>to be installed. Note that this requires <code>python==3.9</code>, and will not install for more recent versions of <code>python</code>.</p> <p>The following should install all (save for <code>python</code> and its packages) on a personal device. For installation on a managed cluster, consult the respective guidelines.</p> Mac OSLinux (Ubuntu/Debian)Linux (Fedora/RHEL) <pre><code># Install brew from https://brew.sh\nbrew install cmake autoconf samtools\n# For non-MPI builds only:\nbrew install hdf5\n# For MPI builds only:\nbrew install openmpi hdf5-mpi\n</code></pre> <pre><code>sudo apt-get update\nsudo apt-get install -y cmake autoconf pkg-config\nsudo apt-get install -y samtools libhts-dev\nsudo apt-get install -y libhdf5-dev\nsudo apt-get install -y libomp-dev\n# For MPI builds only:\nsudo apt-get install -y libopenmpi-dev openmpi-bin\nsudo apt-get install -y libhdf5-openmpi-dev\n</code></pre> <pre><code>sudo dnf install -y cmake autoconf pkg-config\nsudo dnf install -y samtools htslib-devel\nsudo dnf install -y hdf5-devel\nsudo dnf install -y libomp-devel\n# For MPI builds only:\nsudo dnf install -y openmpi-devel hdf5-openmpi-devel\n# Load MPI module (may be required):\nmodule load mpi/openmpi-x86_64\n</code></pre> <p>Verify Dependencies</p> <p>Run these commands to verify the dependencies are successfully installed: </p><pre><code>cmake --version\npkg-config --modversion hdf5\nautoreconf --version\nsamtools --version\nh5ls --version\n</code></pre> For MPI builds, also run: <pre><code>mpirun --version\n</code></pre><p></p>"},{"location":"installation/requirements/#python-dependencies","title":"Python Dependencies","text":"<p>To create a <code>conda</code> environment with all required dependencies, you may run</p> <pre><code>conda create -n cmuts python=3.9\nconda activate cmuts\npip3 install -r requirements.txt\nconda install -c bioconda fastp\n</code></pre> <p>For the <code>cmuts align</code> dependencies required for demultiplexing, also run</p> <pre><code>conda install -c bioconda ultraplex\n</code></pre>"},{"location":"installation/requirements/#hdf5-configuration","title":"HDF5 Configuration","text":"<p>If <code>cmake</code> has trouble finding your HDF5 installation, you can set</p> <pre><code>export HDF5_DIR=/path/to/hdf5/installation\n</code></pre> <p>If installed via brew, the command <code>brew info hdf5</code> may be helpful for finding the desired path.</p> <p>On Linux, HDF5 is typically installed to <code>/usr/lib/x86_64-linux-gnu/hdf5</code> (Debian/Ubuntu) or <code>/usr/lib64</code> (Fedora/RHEL). You can find it with:</p> <pre><code>pkg-config --variable=libdir hdf5\n</code></pre>"},{"location":"usage/cmuts-align/","title":"cmuts align","text":""},{"location":"usage/cmuts-align/#purpose","title":"Purpose","text":"<p><code>cmuts align</code> is a simple wrapper script for performing trimming, demultiplexing, and alignment of one or more FASTQ/BAM files against a reference FASTA file, and sorting of the resulting SAM files. This is required if running <code>cmuts</code> directly from raw sequencing data.</p>"},{"location":"usage/cmuts-align/#usage","title":"Usage","text":""},{"location":"usage/cmuts-align/#alignment","title":"Alignment","text":"<p>By default, <code>cmuts align</code> will perform alignment only, which also includes building the <code>bowtie2</code> index files and sorting the output SAM files. The syntax is</p> <pre><code>cmuts align \\\n  --fasta \"$FASTA\" \\\n  --threads \"$THREADS\" \\\n  --output \"$ALIGNMENTS\" \\\n  \"$FASTQ\"/*.fastq*\n</code></pre> <p>The sorted alignments will be in BAM format and can be found in the directory <code>$ALIGNMENTS</code>, with the same basename as the original FASTQ files.</p> <p>The default is for <code>cmuts align</code> to use <code>--end-to-end</code> alignment. To use <code>--local</code> alignment instead, simply pass the <code>--local</code> argument.</p>"},{"location":"usage/cmuts-align/#paired-end-alignment","title":"Paired-End Alignment","text":"<p>To specify mates for paired-end sequencing, pass the additional FASTQ files to the <code>--pairs</code> argument after the forward reads.</p> <pre><code>cmuts align \\\n  --fasta \"$FASTA\" \\\n  --threads \"$THREADS\" \\\n  --output \"$ALIGNMENTS\" \\\n  \"$FASTQ\"/*.fastq* \\\n  --pairs \"$PAIRS\"/*.fastq*\n</code></pre> <p>Warning</p> <p>The <code>--pairs</code> inputs must come after the forward reads, otherwise <code>cmuts align</code> will not be able to distinguish what you intend to be the mate.</p>"},{"location":"usage/cmuts-align/#demultiplexing","title":"Demultiplexing","text":"<p>Pre-alignment demultiplexing (via <code>ultraplex</code>) can be achieved by passing a CSV of barcodes to the <code>--barcodes</code> argument.</p> <pre><code>cmuts align \\\n  --fasta \"$FASTA\" \\\n  --threads \"$THREADS\" \\\n  --barcodes \"$BARCODES\" \\\n  --output \"$ALIGNMENTS\" \\\n  \"$FASTQ\"/*.fastq*\n</code></pre> <p>The sorted alignments will again be in BAM format under the directory <code>$ALIGNMENTS</code>. However, their names will be given by the respective barcode which they correspond to, not the name of the original FASTQ file.</p> <p>Warning</p> <p>If the barcodes are named in the CSV file, then the outputs will have those as names rather than the respective barcodes (see <code>ultraplex</code> documentation).</p>"},{"location":"usage/cmuts-align/#trimming","title":"Trimming","text":"<p>Pre-alignment trimming (via <code>cutadapt</code>) can be achieved by passing a sequence to either the <code>--trim-5</code> or <code>--trim-3</code> argument, depending on which side the adapter belongs to. Separate adapters can be passed for each.</p> <pre><code>cmuts align \\\n  --fasta \"$FASTA\" \\\n  --threads \"$THREADS\" \\\n  --trim-5 \"$TRIM5\" \\\n  --trim-3 \"$TRIM3\" \\\n  --output \"$ALIGNMENTS\" \\\n  \"$FASTQ\"/*.fastq*\n</code></pre> <p>Trimming does not change the format of the output.</p> <p>Warning</p> <p>If at least one of <code>--trim-5</code> and <code>--trim-3</code> are passed with <code>--barcodes</code> specified, the trimming will occur first.</p>"},{"location":"usage/cmuts-align/#other-arguments","title":"Other Arguments","text":"<p><code>--overwrite</code> : Overwrite the output directories, including any temporary directories used for demultiplexing and trimming.</p> <p><code>--log</code> : Specify the log file to use. Default: <code>cmuts-align.log</code></p>"},{"location":"usage/cmuts-core/","title":"cmuts core","text":""},{"location":"usage/cmuts-core/#purpose","title":"Purpose","text":"<p><code>cmuts core</code> performs the main job of the <code>cmuts</code> pipeline, which is determining the location and type of mutations, insertions, and deletions in a collection of aligned reads.</p>"},{"location":"usage/cmuts-core/#usage","title":"Usage","text":"<p>All modes of <code>cmuts core</code> require two inputs to run:</p> <ol> <li>Reference sequences, stored in a FASTA file,</li> <li>Aligned reads, stored in one or more SAM/BAM/CRAM files</li> </ol> <p>The basic syntax is </p><pre><code>cmuts core \\\n  -f $FASTA \\\n  -o $OUTPUT \\\n  $FILES\n</code></pre><p></p> <p>It is recommended to use default settings alongside the <code>--no-insertions</code> flag when processing chemical probing data.</p> <p>Warning</p> <p>The alignments must be sorted before passing to <code>cmuts</code>. If you are generating them via <code>cmuts align</code>, then they will automatically be sorted for you.</p>"},{"location":"usage/cmuts-core/#modification-counting","title":"Modification Counting","text":"<p>Most uses of <code>cmuts core</code> (without any of the flags specified later) will output an HDF5 file with a dataset of shape \\(n \\times l \\times 4 \\times 7\\). The former two dimensions specify the reference sequence and residue respectively, and the latter two specify the type of modification seen in accordance with the following array:</p> <p></p> <p>The diagonal corresponds to matches, whereas the off-diagonal corresponds to mismatches. The final three columns correspond to deletions, insertions, and termination events respectively.</p> <p>The name of the dataset is <code>counts-1d</code>, and the group to which it belongs corresponds to the name of the input file. For example, the command</p> <pre><code>cmuts core \\\n  -f $FASTA \\\n  -o $OUTPUT \\\n  IN1.bam SUBDIR/IN2.bam\n</code></pre> <p>will create an HDF5 file <code>$OUTPUT</code> with the structure</p> <pre><code>/\n\u251c\u2500\u2500 IN1\n    \u2514\u2500\u2500 counts-1d\n\u2514\u2500\u2500 SUBDIR\n    \u2514\u2500\u2500 IN2\n        \u2514\u2500\u2500 counts-1d\n</code></pre> <p>and both <code>IN1</code> and <code>IN2</code> are datasets as described above. </p>"},{"location":"usage/cmuts-core/#pairwise-modification-counting","title":"Pairwise Modification Counting","text":"<p>The <code>--pairwise</code> flag instructs <code>cmuts core</code> to count pairs of modifications alongside the one-dimensional data. In addition to the above output, the HDF5 file will contain a second dataset <code>counts-2d</code> of shape \\(n \\times l \\times l \\times 2 \\times 2\\). The first dimension specifies the reference sequence, the next two specifies each pair of bases in that sequence, and the final two specify the four entries of the joint Bernoulli distribution</p> \\[ \\begin{pmatrix} p_{00} &amp; p_{01} \\\\ p_{10} &amp; p_{11}. \\end{pmatrix} \\] <p>Again for an example, the command</p> <pre><code>cmuts core \\\n  --pairwise \\\n  -f $FASTA \\\n  -o $OUTPUT \\\n  IN1.bam SUBDIR/IN2.bam\n</code></pre> <p>will create an HDF5 file <code>$OUTPUT</code> with the structure</p> <pre><code>/\n\u251c\u2500\u2500 IN1\n    \u251c\u2500\u2500 counts-1d\n    \u2514\u2500\u2500 counts-2d\n\u2514\u2500\u2500 SUBDIR\n    \u2514\u2500\u2500 IN2\n        \u251c\u2500\u2500 counts-1d\n        \u2514\u2500\u2500 counts-2d\n</code></pre>"},{"location":"usage/cmuts-core/#command-line-options","title":"Command Line Options","text":""},{"location":"usage/cmuts-core/#inputoutput","title":"Input/Output","text":"<p><code>files</code> : The input SAM/BAM/CRAM files.</p> <p><code>-o, --output</code> : The output HDF5 file. (required)</p> <p><code>-f, --fasta</code> : The reference FASTA file. (required)</p> <p><code>--overwrite</code> : Overwrite an existing HDF5 file.</p> <p><code>--rebuild</code> : Rebuild all index files.</p> <p><code>-c, --compression</code> : Compression level of the HDF5 output (0-9). (default: 3)</p> <p><code>--print-every</code> : How often (in seconds) to print statistics. (default: 0.01)</p> <p><code>-v, --verbose</code> : Enable verbose (debug) logging to .cmuts.log file.</p>"},{"location":"usage/cmuts-core/#mode","title":"Mode","text":"<p><code>--pairwise</code> : Compute pairwise modification counts.</p> <p><code>--tokenize</code> : Tokenize the reference sequences.</p>"},{"location":"usage/cmuts-core/#filtering","title":"Filtering","text":"<p><code>--min-mapq</code> : Mapping quality threshold for alignment processing. (default: 10)</p> <p><code>--min-phred</code> : PHRED score threshold for base processing. (default: 10)</p> <p><code>--min-length</code> : Skip reads shorter than this length. (default: 2)</p> <p><code>--max-length</code> : Skip reads longer than this length. (default: 1024)</p> <p><code>--max-hamming</code> : The maximum number of mismatches, insertions, and deletions in a processed read. (default: 1024)</p> <p><code>--secondary</code> : Consider secondary alignments for processing as well.</p> <p><code>--downsample</code> : Limit read depths per reference.</p> <p><code>--ignore-bases</code> : Do not count mismatches or deletions occurring at these bases. Pass as a single string.</p>"},{"location":"usage/cmuts-core/#processing","title":"Processing","text":"<p><code>--max-indel-length</code> : The longest indels to consider. (default: 10)</p> <p><code>--quality-window</code> : Check the quality of each base in a window of this size around each base. (default: 2)</p> <p><code>--collapse</code> : Collapse modifications within this distance of each other in a given read. (default: 2)</p>"},{"location":"usage/cmuts-core/#performance","title":"Performance","text":"<p><code>--chunk-size</code> : The number of references to process at a time per thread. (default: 128)</p>"},{"location":"usage/cmuts-core/#mutation-type-filters","title":"Mutation type filters","text":"<p><code>--no-mismatches</code> : Do not count mismatches as modifications.</p> <p><code>--no-insertions</code> : Do not count insertions as modifications.</p> <p><code>--no-deletions</code> : Do not count deletions as modifications.</p>"},{"location":"usage/cmuts-core/#strand-options","title":"Strand options","text":"<p><code>--no-reverse</code> : Ignore reverse-complemented reads.</p> <p><code>--only-reverse</code> : Use only reverse-complemented reads.</p>"},{"location":"usage/cmuts-core/#ambiguous-deletions","title":"Ambiguous Deletions","text":"<p><code>--uniform-spread</code> : Uniformly spread out ambiguous deletions.</p> <p><code>--no-spread</code> : Do not spread ambiguous deletions.</p> <p><code>--disable-ambiguous</code> : Disable the ambiguous deletion detection algorithm, relying on the deletion provided by the alignment.</p>"},{"location":"usage/cmuts-core/#quality-filtering","title":"Quality filtering","text":"<p><code>--no-match-filter</code> : Do not filter matches based on their PHRED base score.</p> <p><code>--no-insertion-filter</code> : Do not filter insertions based on their PHRED base score.</p> <p><code>--no-deletion-filter</code> : Do not filter deletions based on their PHRED base score.</p>"},{"location":"usage/cmuts-generate/","title":"cmuts generate","text":""},{"location":"usage/cmuts-generate/#purpose","title":"Purpose","text":"<p><code>cmuts generate</code> creates synthetic test data for validating the <code>cmuts</code> pipeline. It generates random reference sequences and simulated aligned reads with known modifications, allowing you to verify that <code>cmuts core</code> correctly identifies mutations, insertions, and deletions.</p>"},{"location":"usage/cmuts-generate/#usage","title":"Usage","text":"<p>The basic syntax is:</p> <pre><code>cmuts generate \\\n  --length 100 \\\n  --references 10 \\\n  --queries 50 \\\n  --out-fasta references.fa \\\n  -o alignments \\\n  --out-h5 expected.h5 \\\n  --min-mapq 10 \\\n  --min-phred 10 \\\n  --max-length 100 \\\n  --max-indel-length 10 \\\n  --quality-window 2 \\\n  --bam\n</code></pre> <p>This generates:</p> <ol> <li>Reference sequences (<code>--out-fasta</code>): Random nucleotide sequences of the specified length</li> <li>Aligned reads (<code>-o</code>/<code>--out</code>): Simulated alignments with random modifications. At least one of <code>--sam</code>, <code>--bam</code>, or <code>--cram</code> is required.</li> <li>Expected counts (<code>--out-h5</code>): Ground truth modification counts for validation</li> </ol> <p>You can then run <code>cmuts core</code> on the generated data and compare its output to the expected counts to verify correctness.</p>"},{"location":"usage/cmuts-generate/#example-workflow","title":"Example Workflow","text":"<pre><code># Generate test data as sorted BAM\ncmuts generate \\\n  --length 200 \\\n  --references 5 \\\n  --queries 100 \\\n  --out-fasta test_refs.fa \\\n  -o test_reads \\\n  --out-h5 expected_counts.h5 \\\n  --min-mapq 10 \\\n  --min-phred 10 \\\n  --max-length 200 \\\n  --max-indel-length 10 \\\n  --quality-window 2 \\\n  --bam\n\n# Run cmuts core on the generated data\ncmuts core \\\n  -f test_refs.fa \\\n  -o actual_counts.h5 \\\n  test_reads.bam\n\n# Compare actual_counts.h5 with expected_counts.h5\n</code></pre>"},{"location":"usage/cmuts-generate/#command-line-options","title":"Command Line Options","text":""},{"location":"usage/cmuts-generate/#generation","title":"Generation","text":"<p><code>--length</code> : The length of the reference sequences. (required)</p> <p><code>--queries</code> : The number of queries to generate per reference. (required)</p> <p><code>--references</code> : The number of references to generate. (required)</p> <p><code>--seed</code> : Random seed for reproducibility. If not provided, uses time-based seed. (default: -1)</p>"},{"location":"usage/cmuts-generate/#output-files","title":"Output files","text":"<p><code>--out-fasta</code> : The file to store the references in. (required)</p> <p><code>-o, --out</code> : The output file for alignments (SAM, BAM, or CRAM). (required)</p> <p><code>--out-h5</code> : The file to store the expected modifications in. (required)</p>"},{"location":"usage/cmuts-generate/#quality-thresholds","title":"Quality thresholds","text":"<p><code>--min-mapq</code> : The minimum quality to consider a read. (required)</p> <p><code>--min-phred</code> : The minimum quality to consider a base. (required)</p> <p><code>--min-length</code> : The smallest query sequences to consider when counting modifications. (default: 2)</p> <p><code>--max-length</code> : The longest query sequences to consider when counting modifications. (required)</p> <p><code>--max-indel-length</code> : Skip indels longer than this. (required)</p> <p><code>--collapse</code> : The minimum number of bases between modifications to consider them consecutive. (default: 2)</p> <p><code>--quality-window</code> : The number of neighbouring bases to consider when calculating PHRED scores. (required)</p>"},{"location":"usage/cmuts-generate/#mutation-type-filters","title":"Mutation type filters","text":"<p><code>--no-mismatches</code> : Do not count mismatches as modifications.</p> <p><code>--no-insertions</code> : Do not count insertions as modifications.</p> <p><code>--no-deletions</code> : Do not count deletions as modifications.</p>"},{"location":"usage/cmuts-generate/#output-format-flags","title":"Output format flags","text":"<p>At least one of the following is required. Multiple can be used together, in which case <code>-o</code> is treated as a base name and the appropriate extensions are appended.</p> <p><code>--sam</code> : Produce a SAM file with MD tags (requires samtools).</p> <p><code>--bam</code> : Produce a sorted BAM file with MD tags (requires samtools).</p> <p><code>--cram</code> : Produce a sorted CRAM file (requires samtools).</p>"},{"location":"usage/cmuts-normalize/","title":"cmuts normalize","text":""},{"location":"usage/cmuts-normalize/#purpose","title":"Purpose","text":"<p><code>cmuts normalize</code> is designed to take modification counts generated by <code>cmuts core</code>, and generate normalized reactivity profiles for each RNA in the probing library. It will also generate additional coverage statistics and figures for visualization of the generated profiles.</p> <p>In the case that <code>cmuts core</code> was run with the <code>--pairwise</code> argument, the 2D counts computed will also be processed, in order to compute modification rate correlations and similar pairwise statistics.</p>"},{"location":"usage/cmuts-normalize/#usage","title":"Usage","text":"<p>To run <code>cmuts normalize</code> requires an HDF5 file of modification counts and the original sequence library in FASTA format. The basic syntax is </p><pre><code>cmuts normalize \\\n  -o \"$OUTPUT\" \\\n  --mod \"$MOD\" \\\n  --nomod \"$NOMOD\" \\\n  --fasta \"$FASTA\"\n  $FILE\n</code></pre> The <code>--mod</code> and <code>--nomod</code> flags specify the HDF5 datasets in which the treated and untreated modification counts are stored, respectively. The latter is optional.<p></p> <p>Tip</p> <p>More than one file can be passed to <code>cmuts normalize</code>. In such a case, the modification counts are summed across the files before normalization. This is useful in the case where the inputs to <code>cmuts core</code> were split across multiple files.</p>"},{"location":"usage/cmuts-normalize/#command-line-options","title":"Command Line Options","text":""},{"location":"usage/cmuts-normalize/#core-options","title":"Core Options","text":"<p><code>-o, --output</code> : Output HDF5 filename (required)</p> <p><code>--mod</code> : Name of the dataset containing treated modification counts (required)</p> <p><code>--nomod</code> : Name of the dataset containing untreated modification counts</p> <p><code>--fasta</code> : FASTA file of probed RNA sequences.</p>"},{"location":"usage/cmuts-normalize/#output-control","title":"Output Control","text":"<p><code>--overwrite</code> : Overwrite existing HDF5 file</p> <p><code>--group</code> : HDF5 group to place the output datasets in (none if not specified)</p>"},{"location":"usage/cmuts-normalize/#profile-computation","title":"Profile Computation","text":"<p><code>--no-insertions</code> : Do not use insertions to compute the reactivity profiles</p> <p><code>--no-deletions</code> : Do not use deletions to compute the reactivity profiles</p>"},{"location":"usage/cmuts-normalize/#normalization-control","title":"Normalization Control","text":"<p><code>--norm {ubr,raw,outlier}</code> : Normalization method (default: ubr)</p> <ul> <li><code>ubr</code>: 90th percentile normalization (default)</li> <li><code>raw</code>: No normalization</li> <li><code>outlier</code>: 2-8% outlier-based normalization</li> </ul> <p><code>--clip-low</code> : Clip negative reactivity values</p> <p><code>--clip-high</code> : Clip reactivity values above 1</p> <p><code>--blank-5p</code> : NaN out this many bases on the 5' end (default: 0)</p> <p><code>--blank-3p</code> : NaN out this many bases on the 3' end (default: 0)</p> <p><code>--blank-cutoff</code> : NaN out any positions with less than this many reads (default: 10)</p> <p><code>--norm-independent</code> : Normalize each profile separately, rather than using experiment-wide statistics </p>"},{"location":"usage/cmuts-normalize/#pairwise-settings","title":"Pairwise Settings","text":"<p><code>--sig</code> : The significance value to use when plotting pairwise modification correlations.</p>"},{"location":"usage/cmuts-normalize/#outputs","title":"Outputs","text":""},{"location":"usage/cmuts-normalize/#files","title":"Files","text":"<p>The output of <code>cmuts normalize</code> will be an HDF5 file with the following structure:</p> <pre><code>/\n\u251c\u2500\u2500 roi-mask\n\u251c\u2500\u2500 SNR\n\u251c\u2500\u2500 error\n\u251c\u2500\u2500 heatmap\n\u251c\u2500\u2500 norm\n\u251c\u2500\u2500 reactivity\n\u2514\u2500\u2500 reads\n</code></pre> <p>roi-mask (Region Of Interest): A boolean array indicating the region of the RNA which is of structural importance (i.e. all but the flanking sequences and low-coverage regions).</p> <p>SNR (Signal-To-Noise): An estimate of the average SNR per nucleotide, based on the computed error, one for each reference.</p> <p>error: The standard error of the mean at each nucleotide.</p> <p>heatmap: The prevalence of each mutation, insertion, deletion, and termination type throughout the entire library.</p> <p>norm: The normalization value used. It will be a scalar unless <code>--norm-independent</code> is passed, in which case each reference has its own norm value.</p> <p>reactivity: The reactivity profiles.</p> <p>reads: The number of reads used to compute each reactivity profile.</p> <p>In the case where 2D data was also present in the input, the output HDF5 file will also contain</p> <pre><code>/\n\u251c\u2500\u2500 correlation\n\u251c\u2500\u2500 mutual-information\n\u2514\u2500\u2500 pairwise-snr\n</code></pre> <p>pairwise-snr: Signal-to-noise ratio for pairwise joint probabilities. Computed as P(i=1, j=1) / SE(P(i=1, j=1)) for each position pair, then averaged across all pairs (excluding diagonal). One value per reference. Higher values indicate more reliable pairwise correlation estimates.</p> <p>Warning</p> <p>If a value was passed to <code>--group</code>, then the above will be contained in an HDF5 group with that name.</p>"},{"location":"usage/cmuts-normalize/#figures","title":"Figures","text":""},{"location":"usage/cmuts-plot/","title":"cmuts plot","text":""},{"location":"usage/cmuts-plot/#purpose","title":"Purpose","text":"<p><code>cmuts plot</code> may be used to plot one or more reactivity profiles stored in HDF5 files.</p>"},{"location":"usage/cmuts-plot/#usage","title":"Usage","text":"<p>The general syntax is </p><pre><code>cmuts plot \\\n  --files FILE1 FILE2 ... \\\n  --datasets DS1 DS2 ... \\\n  --labels LABEL1 LABEL2 ...\n</code></pre> and an example can be found under <code>./examples/plot</code>.<p></p>"},{"location":"usage/cmuts-test/","title":"cmuts test","text":""},{"location":"usage/cmuts-test/#purpose","title":"Purpose","text":"<p><code>cmuts test</code> runs the integration test suite to verify that cmuts is installed correctly and functioning as expected. The tests validate mutation counting accuracy across various scenarios.</p>"},{"location":"usage/cmuts-test/#usage","title":"Usage","text":"<pre><code>cmuts test [OPTIONS]\n</code></pre>"},{"location":"usage/cmuts-test/#options","title":"Options","text":"Option Description <code>--quick</code> Run only fast tests, skipping slow stress tests <code>--cram</code> Run only CRAM format tests <code>-k EXPR</code> Run tests matching the given expression (pytest syntax) <code>-v</code> Increase verbosity"},{"location":"usage/cmuts-test/#examples","title":"Examples","text":"<p>Run all tests: </p><pre><code>cmuts test\n</code></pre><p></p> <p>Run quick tests only (recommended for CI): </p><pre><code>cmuts test --quick\n</code></pre><p></p> <p>Run only CRAM-specific tests: </p><pre><code>cmuts test --cram\n</code></pre><p></p> <p>Run tests matching a pattern: </p><pre><code>cmuts test -k \"edge\"\n</code></pre><p></p>"},{"location":"usage/cmuts-test/#test-categories","title":"Test Categories","text":"<p>The test suite includes several categories of tests:</p>"},{"location":"usage/cmuts-test/#edge-case-tests","title":"Edge Case Tests","text":"<p>Deterministic tests with specific, controlled inputs to verify correct behavior in boundary conditions:</p> <ul> <li>Minimal single-read processing</li> <li>Short reads at minimum length threshold</li> <li>High MAPQ and PHRED filtering</li> <li>Quality window averaging</li> <li>Mutation type filtering (mismatches, insertions, deletions)</li> <li>Collapse distance handling</li> <li>Multiple references</li> </ul>"},{"location":"usage/cmuts-test/#cram-format-tests","title":"CRAM Format Tests","text":"<p>Tests specifically for CRAM file format support:</p> <ul> <li>Basic CRAM processing</li> <li>CRAM with quality filters</li> </ul>"},{"location":"usage/cmuts-test/#random-fuzzing-tests","title":"Random Fuzzing Tests","text":"<p>Tests with randomly generated parameters to catch edge cases:</p> <ul> <li>Random BAM file processing</li> <li>Random CRAM file processing</li> </ul>"},{"location":"usage/cmuts-test/#stress-tests-slow","title":"Stress Tests (Slow)","text":"<p>Large-scale tests marked with <code>@pytest.mark.slow</code>:</p> <ul> <li>Large reference sequences</li> <li>Many references</li> <li>High coverage scenarios</li> <li>Extended fuzzing with many iterations</li> </ul>"},{"location":"usage/cmuts-test/#requirements","title":"Requirements","text":"<p>The test suite requires:</p> <ul> <li><code>pytest</code> (install via <code>pip install pytest</code>)</li> <li><code>samtools</code> (for BAM/CRAM conversion)</li> <li><code>h5py</code> (for HDF5 comparison)</li> <li><code>numpy</code> (for numerical comparison)</li> </ul>"},{"location":"usage/cmuts-test/#output","title":"Output","text":"<p>Tests output a summary showing passed/failed tests:</p> <pre><code>============================= test session starts ==============================\n...\ntests/python/test_core.py::TestEdgeCases::test_minimal_single_read PASSED\ntests/python/test_core.py::TestEdgeCases::test_short_reads_at_min_length PASSED\n...\n============================= 85 passed in 15.10s ==============================\n</code></pre>"},{"location":"usage/cmuts-test/#validation-criteria","title":"Validation Criteria","text":"<p>Each test validates that:</p> <ol> <li>Counts match: The mutation counts from <code>cmuts core</code> match the expected counts generated by <code>cmuts generate</code></li> <li>No crashes: Processing completes without errors</li> <li>Correct filtering: Quality and length filters are applied correctly</li> </ol>"},{"location":"usage/cmuts-visualize/","title":"cmuts visualize","text":""},{"location":"usage/cmuts-visualize/#purpose","title":"Purpose","text":"<p><code>cmuts visualize</code> may be used to overlay a reactivity profile generated via <code>cmuts</code> onto a 3D RNA structure.</p>"},{"location":"usage/cmuts-visualize/#usage","title":"Usage","text":"<p>The general syntax is </p><pre><code>cmuts visualize \\\n  --file \"$PROFILES\" \\\n  --dataset \"$NAME\" \\\n  --fasta \"$FASTA\" \\\n  --cif \"$CIF\"\n</code></pre> and an example can be found under <code>./examples/visualize</code>.<p></p> <p>The FASTA file is necessary to perform alignment of the sequence which was probed against the sequence of the 3D structure, which may be different e.g. due to flanking sequences.</p>"},{"location":"usage/cmuts-visualize/#command-line-options","title":"Command Line Options","text":""},{"location":"usage/cmuts-visualize/#core-options","title":"Core Options","text":"<p><code>--file</code> : Path to the HDF5 file containing the reactivity data</p> <p><code>--dataset</code> : Reactivity dataset name in the HDF5 file</p> <p><code>--fasta</code> : Path to .fasta file of the library which was chemically probed</p> <p><code>--cif</code> : Path to the .cif structure file (.pdb also works)</p> <p><code>--bin</code> : Path to the ChimeraX executable (default: ChimeraX)</p>"},{"location":"usage/cmuts-visualize/#selection-options","title":"Selection Options","text":"<p><code>--index</code> : Index of the sequence in the FASTA and the reactivity in the HDF5 file (default: 0)</p> <p><code>--chain</code> : Color this chain of the structure (default: ALL)</p> <p><code>--trim-5p</code> : Trim this many bases from the 5' end of the reactivity data (default: 0)</p> <p><code>--trim-3p</code> : Trim this many bases from the 3' end of the reactivity data (default: 0)</p>"},{"location":"usage/cmuts-visualize/#output-options","title":"Output Options","text":"<p><code>--color</code> : The ChimeraX color to use for highly reactive bases (default: indianred)</p>"},{"location":"usage/cmuts-visualize/#movies","title":"Movies","text":"<p>Once the structure is loaded into ChimeraX, you can record a movie of it rotating by running</p> <pre><code>movie record\nturn y 0.5 720\nmovie stop\nmovie encode PATH-TO-MOVIE\n</code></pre>"},{"location":"usage/cmuts/","title":"Overview","text":"<p>The functionality of <code>cmuts</code> is distributed among several subprograms. All accept the <code>--log</code> argument to redirect their outputs to a log file.</p> <pre><code>cmuts [--log LOGFILE] {align|core|normalize|visualize|test}\n</code></pre> <p>The main <code>cmuts</code> pipeline comprises the three subprograms</p> <ol> <li><code>cmuts align</code>,</li> <li><code>cmuts core</code>, and</li> <li><code>cmuts normalize</code>.</li> </ol> <p>Warning</p> <p>Some sequencing vendors (e.g. Ultima) will provide pre-aligned BAM/CRAM files. In such a case, simply ignore the <code>cmuts align</code> step.</p> <p>See Examples for detailed usage instructions.</p> <p><code>cmuts visualize</code> may be used for overlaying generated reactivities on tertiary structures.</p> <p><code>cmuts plot</code> may be used for plotting reactivity profiles.</p> <p><code>cmuts test</code> runs the integration test suite to verify installation.</p>"},{"location":"usage/examples/","title":"Examples","text":""},{"location":"usage/examples/#overview","title":"Overview","text":""},{"location":"usage/examples/#environment-setup","title":"Environment Setup","text":"<p>These variables will be used throughout the examples; feel free to set them to a value appropriate for your computer. The latter three specify the location of various intermediate files and outputs from the pipeline.</p> <pre><code>THREADS=8\nALIGNMENTS=\"examples/alignments\"\nCOUNTS=\"examples/counts.h5\"\nPROFILES=\"examples/profiles.h5\"\n</code></pre> <p>All files used in these examples can be found in the repo, under <code>examples</code>.</p>"},{"location":"usage/examples/#map-seq","title":"MaP-seq","text":"<pre><code>PARENT=\"examples/map-seq\"\n</code></pre> <p>A directory of FASTQ files and a reference FASTA file is required.</p> <pre><code>FASTQ=\"$PARENT/fastq\"\nFASTA=\"$PARENT/ref.fasta\"\n</code></pre> <p>In addition, <code>cmuts normalize</code> will need to know which datasets correspond to which condition. How this is specified depends on whether demultiplexing is performed or not.</p>"},{"location":"usage/examples/#without-demultiplexing","title":"Without Demultiplexing","text":"<p>The following arrays contain the basenames of the FASTQ files corresponding to each condition. A parallel array containing the name of each condition is necessary as well.</p> <pre><code>MODS=(\"bicine-2A3\", \"bicine-DMS\")\nNOMODS=(\"bicine-DMSO\", \"bicine-ETH\")\nNAMES=(\"2A3\", \"DMS\")\n</code></pre> <p><code>NOMODS</code> may be empty, in the case where background subtraction is not to be performed.</p> <p>One can run <code>cmuts align</code> directly on the FASTQ files and then pass the output to <code>cmuts core</code> to locate modifications.</p> <pre><code>cmuts align \\\n  --fasta \"$FASTA\" \\\n  --threads \"$THREADS\" \\\n  --output \"$ALIGNMENTS\" \\\n  \"$FASTQ\"/*.fastq*\n\ncmuts core \\\n  -f \"$FASTA\" \\\n  -o \"$COUNTS\" \\\n  --no-insertions \\\n  \"$ALIGNMENTS\"/*\n</code></pre> <p>This will output a single HDF5 file, <code>$COUNTS</code>, which will contain one dataset for each input. The following statistics should also be printed:</p> <pre><code>        cmuts version 1.0.1\n      \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        References:                    1\n        Reference length:            615\n        Aligned reads:             9,965\n        Unaligned reads:              35\n      \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        File:                        4/4\n        Reads processed:          100.0%\n        Reads skipped:             18.3%\n        Time elapsed:           00:00:00\n      \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre> <p>One can then loop over all input files, and pass the respective dataset to <code>cmuts normalize</code>.</p> <pre><code>for ((IX=0; IX&lt;${#MODS[@]}; IX++)); do\n  MOD_DS=\"$ALIGNMENTS\"/${MODS[IX]}\n  NOMOD_DS=\"$ALIGNMENTS\"/${NOMODS[IX]}\n  NAME=${NAMES[IX]}\n  cmuts normalize \\\n    -o \"$PROFILES\" \\\n    --mod \"$MOD_DS\" \\\n    --nomod \"$NOMOD_DS\" \\\n    --group \"$NAME\" \\\n    \"$COUNTS\"\ndone\n</code></pre> <p>The following will also be printed:</p> <pre><code>        cmuts-normalize version 1.0.1\n      \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        Statistics for 2A3:\n        References:       1\n        Total reads:      3,088\n        Mean reads:       3088.00\n        Mean SNR:         1.28\n        Mean reactivity:  0.006\n        Median reads:     3,088\n        Median SNR:       1.28\n        Mean-to-Median:   1.00\n        Usable SNR:       1.00\n        Dropout Fraction: 0.00\n\n        cmuts-normalize version 1.0.1\n      \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        Statistics for DMS:\n        References:       1\n        Total reads:      3,716\n        Mean reads:       3716.00\n        Mean SNR:         2.51\n        Mean reactivity:  0.012\n        Median reads:     3,716\n        Median SNR:       2.51\n        Mean-to-Median:   1.00\n        Usable SNR:       1.00\n        Dropout Fraction: 0.00\n</code></pre> <p>The output HDF5 file will have the following structure:</p> <pre><code>/\n\u251c\u2500\u2500 2A3/\n\u2502   \u251c\u2500\u2500 ROI\n\u2502   \u251c\u2500\u2500 SNR\n\u2502   \u251c\u2500\u2500 error\n\u2502   \u251c\u2500\u2500 heatmap\n\u2502   \u251c\u2500\u2500 norm\n\u2502   \u251c\u2500\u2500 reactivity\n\u2502   \u2514\u2500\u2500 reads\n\u2514\u2500\u2500 DMS/\n    \u251c\u2500\u2500 ROI\n    \u251c\u2500\u2500 SNR\n    \u251c\u2500\u2500 error\n    \u251c\u2500\u2500 heatmap\n    \u251c\u2500\u2500 norm\n    \u251c\u2500\u2500 reactivity\n    \u2514\u2500\u2500 reads\n</code></pre> <p>If you wish to overlay the reactivity profile onto a 3D structure stored in <code>CIF</code>, then you may then run</p> <pre><code>for ((IX=0; IX&lt;${#NAMES[@]}; IX++)); do\n  NAME=${NAMES[IX]}\n  cmuts visualize \\\n    --file \"$PROFILES\" \\\n    --dataset \"$NAME\" \\\n    --fasta \"$FASTA\" \\\n    --cif \"$CIF\"\ndone\n</code></pre>"},{"location":"usage/examples/#with-demultiplexing","title":"With Demultiplexing","text":"<pre><code>PARENT=\"examples/map-seq-dmux\"\n</code></pre> <p>The main difference when working with FASTQ files which must be demultiplexed first is that <code>MODS</code> and <code>NOMODS</code> should refer to the barcodes for each condition, rather than the files themselves.</p> <pre><code>MODS=(\"GCCTGGGTGGCT\", \"TGACCATGTATA\")\nNOMODS=(\"AAGGACCACTGG\", \"CTTATTACACAC\")\nNAMES=(\"2A3\", \"DMS\")\n</code></pre> <p>We use a different directory of FASTQ files for this example. Naturally, in addition to the previous inputs, a comma-separated file containing the barcodes to use for demultiplexing must be provided to <code>cmuts align</code>.</p> <pre><code>FASTQ=\"$PARENT/fastq\"\nFASTA=\"$PARENT/ref.fasta\"\nBARCODES=\"$PARENT/barcodes.fasta\"\n</code></pre> <p>Otherwise, the pipeline remains the same:</p> <pre><code>cmuts align \\\n  --fasta \"$FASTA\" \\\n  --threads \"$THREADS\" \\\n  --barcodes \"$BARCODES\" \\\n  --output \"$ALIGNMENTS\" \\\n  \"$FASTQ\"/*.fastq*\n\ncmuts core \\\n  -f \"$FASTA\" \\\n  -o \"$COUNTS\" \\\n  --no-insertions \\\n  \"$ALIGNMENTS\"/*\n\nfor ((IX=0; IX&lt;${#MODS[@]}; IX++)); do\n  MOD_DS=\"$ALIGNMENTS\"/${MODS[IX]}\n  NOMOD_DS=\"$ALIGNMENTS\"/${NOMODS[IX]}\n  NAME=${NAMES[IX]}\n  cmuts normalize \\\n    -o \"$PROFILES\" \\\n    --mod \"$MOD_DS\" \\\n    --nomod \"$NOMOD_DS\" \\\n    --group \"$NAME\" \\\n    \"$COUNTS\"\ndone\n</code></pre>"},{"location":"usage/examples/#two-dimensional-map-seq","title":"Two-Dimensional MaP-seq","text":""},{"location":"usage/examples/#residue-residue-correlations","title":"Residue-Residue Correlations","text":"<pre><code>PARENT=\"examples/mohca\"\n</code></pre> <p>This example uses MOHCA-seq data.</p> <pre><code>FASTQ=\"$PARENT/fastq\"\nFASTA=\"$PARENT/ref.fasta\"\n</code></pre> <p>The first two steps are the same, save for the <code>--pairwise</code> flag passed to <code>cmuts core</code>.</p> <pre><code>cmuts align \\\n  --fasta \"$FASTA\" \\\n  --threads \"$THREADS\" \\\n  --output \"$ALIGNMENTS\" \\\n  \"$FASTQ\"/*.fastq*\n\ncmuts core \\\n  --pairwise \\\n  -f \"$FASTA\" \\\n  -o \"$COUNTS\" \\\n  --no-insertions \\\n  \"$ALIGNMENTS\"/*\n</code></pre> <p>The third step uses <code>cmuts normalize</code> to postprocess the 2D data.</p> <pre><code>for ((IX=0; IX&lt;${#MODS[@]}; IX++)); do\n  MOD_DS=\"$ALIGNMENTS\"/${MODS[IX]}\n  cmuts normalize \\\n    -o \"$PROFILES\" \\\n    --mod \"$MOD_DS\" \\\n    --clip-low \\\n    \"$COUNTS\"\ndone\n</code></pre>"},{"location":"usage/examples/#mutate-and-map","title":"Mutate-And-Map","text":"<pre><code>PARENT=\"examples/m2\"\n</code></pre> <pre><code>FASTQ=\"$PARENT/fastq\"\nFASTA=\"$PARENT/ref.fasta\"\n</code></pre>"},{"location":"usage/pipeline/","title":"Pipeline","text":"<p>The <code>cmuts</code> pipeline consists of three main programs:</p> <ol> <li><code>cmuts align</code> - Alignment and preprocessing wrapper (uses bowtie2, samtools, fastp)</li> <li><code>cmuts core</code> - Mutation counting engine (use <code>--pairwise</code> flag for 2D MaP-seq data)</li> <li><code>cmuts normalize</code> - Normalization and reactivity profile generation (handles both 1D and 2D data automatically)</li> </ol> <p>Warning</p> <p>Some sequencing vendors (e.g. Ultima) will provide pre-aligned BAM/CRAM files. In such a case, simply ignore the <code>cmuts align</code> step.</p>"},{"location":"usage/pipeline/#optional-tools","title":"Optional Tools","text":"<ul> <li><code>cmuts plot</code> - Plot reactivity profiles from HDF5 files</li> <li><code>cmuts visualize</code> - Overlay reactivities on 3D structures (requires ChimeraX)</li> <li><code>cmuts generate</code> - Generate synthetic test data for validation</li> <li><code>cmuts test</code> - Run the integration test suite</li> </ul>"}]}